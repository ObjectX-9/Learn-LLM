import { NextRequest } from 'next/server';
import { ChatOpenAI } from '@langchain/openai';
import {
  ChatPromptTemplate,
  SystemMessagePromptTemplate,
  HumanMessagePromptTemplate,
} from '@langchain/core/prompts';
import { StringOutputParser } from '@langchain/core/output_parsers';

// å®šä¹‰æ¶ˆæ¯ç±»å‹æ¥å£
interface WebSocketMessage {
  type:
    | 'ping'
    | 'chat'
    | 'data-stream'
    | 'notification'
    | 'log-stream'
    | 'broadcast'
    | 'custom';
  payload: any;
}

interface ChatPayload {
  message: string;
  system?: string;
  temperature?: number;
  modelName?: string;
}

interface DataStreamPayload {
  duration?: number;
  interval?: number;
}

// å­˜å‚¨æ´»è·ƒçš„è¿æ¥ï¼ˆå®é™…é¡¹ç›®ä¸­åº”è¯¥ä½¿ç”¨å¤–éƒ¨å­˜å‚¨ï¼‰
const activeConnections = new Set<WebSocket>();

export async function GET(request: NextRequest) {
  console.log('ğŸš€ WebSocket API çŠ¶æ€æ£€æŸ¥:', new Date().toISOString());

  const searchParams = request.nextUrl.searchParams;
  const type = searchParams.get('type') || 'status';

  // è¿”å›WebSocketæœåŠ¡çŠ¶æ€ä¿¡æ¯
  return Response.json({
    status: 'ready',
    message: 'WebSocket APIæœåŠ¡å·²å‡†å¤‡å°±ç»ª',
    timestamp: Date.now(),
    type: type,
    features: {
      aiChat: true,
      dataStream: true,
      notifications: true,
      logStream: true,
      heartbeat: true,
    },
    endpoints: {
      websocket: process.env.WEBSOCKET_URL || 'ws://localhost:3001',
      fallback: '/api/streaming/socket',
    },
  });
}

// å¤„ç†WebSocketæ¶ˆæ¯çš„æ ¸å¿ƒå‡½æ•°
export async function POST(request: NextRequest) {
  const body = (await request.json()) as WebSocketMessage;
  const { type, payload } = body;

  console.log('ğŸ“© å¤„ç†WebSocketæ¶ˆæ¯:', { type, payload });

  try {
    switch (type) {
      case 'ping':
        return handlePing(payload);

      case 'chat':
        return await handleChatMessage(payload as ChatPayload);

      case 'data-stream':
        return await handleDataStream(payload as DataStreamPayload);

      case 'notification':
        return handleNotification(payload);

      case 'log-stream':
        return handleLogStream(payload);

      case 'broadcast':
        return handleBroadcast(payload);

      case 'custom':
        return handleCustomMessage(payload);

      default:
        return Response.json(
          {
            type: 'error',
            message: `æœªçŸ¥æ¶ˆæ¯ç±»å‹: ${type}`,
            timestamp: Date.now(),
          },
          { status: 400 }
        );
    }
  } catch (error) {
    console.error('âŒ æ¶ˆæ¯å¤„ç†é”™è¯¯:', error);
    const errorMessage = error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯';

    return Response.json(
      {
        type: 'error',
        message: 'æ¶ˆæ¯å¤„ç†å¤±è´¥',
        error: errorMessage,
        timestamp: Date.now(),
      },
      { status: 500 }
    );
  }
}

// å¤„ç†å¿ƒè·³æ£€æµ‹
function handlePing(payload: any) {
  return Response.json({
    type: 'pong',
    payload: {
      timestamp: Date.now(),
      originalTimestamp: payload?.timestamp || null,
    },
  });
}

// å¤„ç†AIèŠå¤©æ¶ˆæ¯ - ä½¿ç”¨LangChainå’ŒTypeScript
async function handleChatMessage(payload: ChatPayload): Promise<Response> {
  const {
    message,
    system = 'You are a helpful AI assistant. Respond in a conversational and friendly manner.',
    temperature = 0.7,
    modelName = 'gpt-3.5-turbo',
  } = payload;

  console.log('ğŸ¤– å¼€å§‹å¤„ç†AIèŠå¤©:', { message, modelName, temperature });

  // éªŒè¯ç¯å¢ƒå˜é‡
  if (!process.env.OPEN_API_KEY) {
    return Response.json(
      {
        type: 'error',
        message: 'âŒ æœªé…ç½®OpenAI APIå¯†é’¥',
      },
      { status: 500 }
    );
  }

  try {
    // åˆå§‹åŒ–ChatOpenAI
    const llm = new ChatOpenAI({
      openAIApiKey: process.env.OPEN_API_KEY,
      modelName: modelName,
      temperature: temperature,
      maxTokens: 2000,
      configuration: {
        baseURL: process.env.OPEN_API_BASE_URL,
      },
      verbose: true,
    });

    // åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿
    const chatPrompt = ChatPromptTemplate.fromMessages([
      SystemMessagePromptTemplate.fromTemplate(system),
      HumanMessagePromptTemplate.fromTemplate('{userMessage}'),
    ]);

    // åˆ›å»ºå¤„ç†é“¾
    const chain = chatPrompt.pipe(llm).pipe(new StringOutputParser());

    console.log('ğŸ”— å¼€å§‹è°ƒç”¨LLM...');

    // éæµå¼è°ƒç”¨ï¼ˆé€‚åˆWebSocketåœºæ™¯ï¼‰
    const response = await chain.invoke({
      userMessage: message,
    });

    console.log('âœ… LLMå“åº”å®Œæˆ');

    // è¿”å›èŠå¤©ç»“æœ
    return Response.json({
      type: 'chat-complete',
      payload: {
        message: response,
        originalMessage: message,
        model: modelName,
        temperature: temperature,
        timestamp: Date.now(),
        stats: {
          responseLength: response.length,
          processingTime: Date.now(),
        },
      },
    });
  } catch (error) {
    console.error('âŒ LLMè°ƒç”¨é”™è¯¯:', error);
    const errorMessage = error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯';

    return Response.json(
      {
        type: 'chat-error',
        message: `AIå¤„ç†å¤±è´¥: ${errorMessage}`,
        originalMessage: message,
        timestamp: Date.now(),
      },
      { status: 500 }
    );
  }
}

// å¤„ç†æµå¼AIèŠå¤©ï¼ˆè¿”å›ReadableStreamï¼‰
export async function PATCH(request: NextRequest) {
  const body = (await request.json()) as ChatPayload;
  const {
    message,
    system = 'You are a helpful AI assistant. Respond in a conversational and friendly manner.',
    temperature = 0.7,
    modelName = 'gpt-3.5-turbo',
  } = body;

  console.log('ğŸŒŠ å¼€å§‹å¤„ç†æµå¼AIèŠå¤©:', { message, modelName });

  const encoder = new TextEncoder();

  const readable = new ReadableStream({
    async start(controller) {
      try {
        // å‘é€å¼€å§‹çŠ¶æ€
        controller.enqueue(
          encoder.encode(
            `data: ${JSON.stringify({
              type: 'chat-start',
              payload: { message: 'ğŸ¤– æ­£åœ¨æ€è€ƒæ‚¨çš„é—®é¢˜...' },
            })}\n\n`
          )
        );

        // åˆå§‹åŒ–ChatOpenAIï¼ˆæµå¼æ¨¡å¼ï¼‰
        const llm = new ChatOpenAI({
          openAIApiKey: process.env.OPEN_API_KEY,
          modelName: modelName,
          temperature: temperature,
          maxTokens: 2000,
          streaming: true,
          configuration: {
            baseURL: process.env.OPEN_API_BASE_URL,
          },
        });

        // åˆ›å»ºèŠå¤©æç¤ºæ¨¡æ¿
        const chatPrompt = ChatPromptTemplate.fromMessages([
          SystemMessagePromptTemplate.fromTemplate(system),
          HumanMessagePromptTemplate.fromTemplate('{userMessage}'),
        ]);

        // åˆ›å»ºå¤„ç†é“¾
        const chain = chatPrompt.pipe(llm).pipe(new StringOutputParser());

        // æµå¼è°ƒç”¨
        const stream = await chain.stream({
          userMessage: message,
        });

        let totalTokens = 0;
        let chunkCount = 0;

        for await (const chunk of stream) {
          chunkCount++;
          totalTokens += chunk.length;

          // å‘é€æµå¼å†…å®¹
          controller.enqueue(
            encoder.encode(
              `data: ${JSON.stringify({
                type: 'chat-stream',
                payload: {
                  content: chunk,
                  chunkCount,
                  totalTokens,
                },
              })}\n\n`
            )
          );
        }

        // å‘é€å®ŒæˆçŠ¶æ€
        controller.enqueue(
          encoder.encode(
            `data: ${JSON.stringify({
              type: 'chat-complete',
              payload: {
                message: 'âœ… å›ç­”ç”Ÿæˆå®Œæˆ',
                stats: {
                  chunks: chunkCount,
                  tokens: totalTokens,
                  model: modelName,
                },
              },
            })}\n\n`
          )
        );

        controller.close();
      } catch (error) {
        console.error('âŒ æµå¼èŠå¤©é”™è¯¯:', error);
        const errorMessage =
          error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯';

        controller.enqueue(
          encoder.encode(
            `data: ${JSON.stringify({
              type: 'chat-error',
              message: `âŒ ç”Ÿæˆå›ç­”æ—¶å‡ºé”™: ${errorMessage}`,
            })}\n\n`
          )
        );

        controller.close();
      }
    },
  });

  return new Response(readable, {
    headers: {
      'Content-Type': 'text/event-stream',
      'Cache-Control': 'no-cache',
      Connection: 'keep-alive',
      'Access-Control-Allow-Origin': '*',
    },
  });
}

// å¤„ç†æ•°æ®æµ
async function handleDataStream(payload: DataStreamPayload): Promise<Response> {
  const { duration = 15000, interval = 200 } = payload;

  console.log('ğŸ“Š å¼€å§‹æ•°æ®æµæ¼”ç¤º:', { duration, interval });

  // ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
  const dataPoints = [];
  const totalPoints = Math.floor(duration / interval);

  for (let i = 0; i < Math.min(totalPoints, 50); i++) {
    const progress = i / totalPoints;
    dataPoints.push({
      timestamp: Date.now() + i * interval,
      value: Math.sin(progress * Math.PI * 4) * 50 + 50 + Math.random() * 10,
      cpu: Math.random() * 100,
      memory: Math.random() * 100,
      network: Math.random() * 1000,
      progress: Math.round(progress * 100),
    });
  }

  return Response.json({
    type: 'data-stream-batch',
    payload: {
      data: dataPoints,
      totalPoints: dataPoints.length,
      duration: duration,
      interval: interval,
    },
  });
}

// å¤„ç†é€šçŸ¥
function handleNotification(payload: any): Response {
  const notifications = [
    { type: 'info', title: 'ç³»ç»Ÿé€šçŸ¥', message: 'æ¬¢è¿ä½¿ç”¨ WebSocket é€šçŸ¥ç³»ç»Ÿ' },
    { type: 'success', title: 'æ“ä½œæˆåŠŸ', message: 'æ•°æ®å·²æˆåŠŸä¿å­˜åˆ°æ•°æ®åº“' },
    {
      type: 'warning',
      title: 'æ³¨æ„',
      message: 'ç³»ç»Ÿèµ„æºä½¿ç”¨ç‡è¾ƒé«˜ï¼Œè¯·æ³¨æ„ç›‘æ§',
    },
    { type: 'error', title: 'é”™è¯¯', message: 'ç½‘ç»œè¿æ¥å‡ºç°é—®é¢˜ï¼Œæ­£åœ¨è‡ªåŠ¨é‡è¯•' },
    { type: 'info', title: 'æ›´æ–°', message: 'å‘ç°æ–°ç‰ˆæœ¬ v2.1.0ï¼Œå»ºè®®ç«‹å³æ›´æ–°' },
  ];

  const randomNotification =
    notifications[Math.floor(Math.random() * notifications.length)];

  return Response.json({
    type: 'notification',
    payload: {
      ...randomNotification,
      id: Date.now(),
      timestamp: Date.now(),
    },
  });
}

// å¤„ç†æ—¥å¿—æµ
function handleLogStream(payload: any): Response {
  const logs = [
    { level: 'info', message: 'ç³»ç»Ÿå¯åŠ¨ä¸­...' },
    { level: 'debug', message: 'åŠ è½½é…ç½®æ–‡ä»¶ config.json' },
    { level: 'info', message: 'æ•°æ®åº“è¿æ¥å·²å»ºç«‹ (MongoDB)' },
    { level: 'warn', message: 'æ£€æµ‹åˆ°é«˜å»¶è¿Ÿï¼Œå½“å‰å»¶è¿Ÿ 250ms' },
    { level: 'info', message: 'å¤„ç†ç”¨æˆ·è¯·æ±‚ GET /api/users' },
    { level: 'debug', message: 'æ‰§è¡ŒæŸ¥è¯¢æ“ä½œ db.users.find()' },
    { level: 'error', message: 'è¿æ¥è¶…æ—¶ï¼Œæ­£åœ¨é‡è¯•... (1/3)' },
    { level: 'info', message: 'é‡è¿æˆåŠŸï¼Œè¿æ¥å·²æ¢å¤' },
    { level: 'debug', message: 'æ•°æ®å¤„ç†å®Œæˆï¼Œè¿”å› 156 æ¡è®°å½•' },
    { level: 'info', message: 'å“åº”å·²å‘é€ï¼Œè€—æ—¶ 423ms' },
  ];

  const batchLogs = logs.slice(0, 5).map((log, index) => ({
    ...log,
    timestamp: Date.now() + index * 100,
    id: Date.now() + index,
  }));

  return Response.json({
    type: 'log-stream-batch',
    payload: {
      logs: batchLogs,
      totalLogs: batchLogs.length,
    },
  });
}

// å¤„ç†å¹¿æ’­æ¶ˆæ¯
function handleBroadcast(payload: any): Response {
  console.log('ğŸ“¢ å¤„ç†å¹¿æ’­æ¶ˆæ¯:', payload);

  return Response.json({
    type: 'broadcast',
    payload: {
      message: payload.message || 'è¿™æ˜¯ä¸€æ¡å¹¿æ’­æ¶ˆæ¯',
      from: 'server',
      timestamp: Date.now(),
      connectionCount: activeConnections.size,
    },
  });
}

// å¤„ç†è‡ªå®šä¹‰æ¶ˆæ¯
function handleCustomMessage(payload: any): Response {
  console.log('ğŸ”§ å¤„ç†è‡ªå®šä¹‰æ¶ˆæ¯:', payload);

  return Response.json({
    type: 'custom-response',
    payload: {
      originalMessage: payload.message || payload,
      response: `æœåŠ¡å™¨æ”¶åˆ°æ‚¨çš„æ¶ˆæ¯: "${payload.message || JSON.stringify(payload)}"`,
      timestamp: Date.now(),
    },
  });
}
