import { NextRequest, NextResponse } from 'next/server';
import { ChatOpenAI } from '@langchain/openai';
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  SystemMessagePromptTemplate,
} from '@langchain/core/prompts';
import { StringOutputParser } from '@langchain/core/output_parsers';

// CoT è¯·æ±‚æ¥å£
export interface CoTRequest {
  question: string;
  context?: string;
  complexity?: 'simple' | 'medium' | 'complex';
  domain?: string;
  temperature?: number;
  maxTokens?: number;
  modelName?: string;
  stream?: boolean;
  showSteps?: boolean;
}

// CoT å“åº”æ¥å£
export interface CoTResponse {
  question: string;
  analysis: string;
  reasoning_steps: string[];
  conclusion: string;
  confidence: number;
  total_steps: number;
  execution_time?: number;
}

export async function POST(request: NextRequest) {
  const startTime = Date.now();

  try {
    const body = await request.json();

    // å…¼å®¹PromptTestBaseçš„è¯·æ±‚æ ¼å¼
    const question = body.question || body.prompt;
    const context = body.context || '';
    const complexity = body.complexity || 'medium';
    const domain = body.domain || 'é€šç”¨';
    const temperature = body.temperature || 0.3;
    const maxTokens = body.maxTokens || 2000;
    const modelName = body.modelName || 'gpt-4';
    const stream = body.stream || false;

    // éªŒè¯è¾“å…¥
    if (!question || question.trim().length === 0) {
      return NextResponse.json({ error: 'é—®é¢˜ä¸èƒ½ä¸ºç©º' }, { status: 400 });
    }

    // åˆ›å»º ChatOpenAI å®ä¾‹
    const llm = new ChatOpenAI({
      openAIApiKey: process.env.OPEN_API_KEY,
      modelName,
      temperature,
      maxTokens,
      configuration: {
        baseURL: process.env.OPEN_API_BASE_URL,
      },
      timeout: 30000,
    });

    if (stream) {
      // çœŸæ­£çš„æµå¼å“åº”å¤„ç†
      return await createStreamingResponse(
        llm,
        question,
        context,
        complexity,
        domain
      );
    } else {
      // éæµå¼å“åº”
      const result = await performCoTReasoning(
        llm,
        question,
        context,
        complexity,
        domain
      );
      const executionTime = Date.now() - startTime;

      return NextResponse.json({
        ...result,
        execution_time: executionTime,
      });
    }
  } catch (error) {
    console.error('CoT API Error:', error);
    return NextResponse.json(
      {
        error: 'å¤„ç†é“¾å¼æ€è€ƒè¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯',
        details: error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯',
      },
      { status: 500 }
    );
  }
}

// åˆ›å»ºçœŸæ­£çš„æµå¼å“åº”
async function createStreamingResponse(
  llm: ChatOpenAI,
  question: string,
  context: string,
  complexity: string,
  domain: string
) {
  const encoder = new TextEncoder();

  const readable = new ReadableStream({
    async start(controller) {
      try {
        console.log('=== å¼€å§‹é“¾å¼æ€è€ƒæ¨ç†æµç¨‹ ===');
        console.log('å‚æ•°:', { question, context, complexity, domain });

        // å‘é€å¼€å§‹ä¿¡æ¯
        const startMsg = {
          content: `# ğŸ§  é“¾å¼æ€è€ƒæ¨ç†\n\n## ğŸ“‹ åŸé—®é¢˜\n${question}\n\n`,
          done: false,
        };
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify(startMsg)}\n\n`)
        );

        // æ­¥éª¤1ï¼šé—®é¢˜åˆ†æ
        console.log('=== æ­¥éª¤1: é—®é¢˜åˆ†æ ===');
        await streamAnalysis(
          controller,
          encoder,
          llm,
          question,
          context,
          complexity,
          domain
        );
        console.log('é—®é¢˜åˆ†ææ­¥éª¤å®Œæˆ');

        // æ­¥éª¤2ï¼šé€æ­¥æ¨ç†
        console.log('=== æ­¥éª¤2: é€æ­¥æ¨ç† ===');
        const analysisResult = await streamReasoning(
          controller,
          encoder,
          llm,
          question,
          complexity,
          domain
        );
        console.log('é€æ­¥æ¨ç†æ­¥éª¤å®Œæˆ');

        // æ­¥éª¤3ï¼šç»“è®ºæ€»ç»“
        console.log('=== æ­¥éª¤3: ç»“è®ºæ€»ç»“ ===');
        await streamConclusion(
          controller,
          encoder,
          llm,
          question,
          analysisResult
        );
        console.log('ç»“è®ºæ€»ç»“æ­¥éª¤å®Œæˆ');

        // å‘é€å®Œæˆä¿¡æ¯
        const completionMsg = {
          content: `\n\nâœ… **æ¨ç†å®Œæˆ**`,
          done: true,
        };
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify(completionMsg)}\n\n`)
        );

        console.log('=== é“¾å¼æ€è€ƒæ¨ç†æµç¨‹å…¨éƒ¨å®Œæˆ ===');
        controller.close();
      } catch (error) {
        console.error('=== é“¾å¼æ€è€ƒæ¨ç†æµç¨‹å‡ºç°é”™è¯¯ ===', error);

        const errorMsg = {
          content: `\n\nâŒ **å¤„ç†è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯**: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}\n\nè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥å’ŒAPIé…ç½®ï¼Œæˆ–ç¨åé‡è¯•ã€‚`,
          done: true,
        };
        controller.enqueue(
          encoder.encode(`data: ${JSON.stringify(errorMsg)}\n\n`)
        );
        controller.close();
      }
    },
  });

  return new Response(readable, {
    headers: {
      'Content-Type': 'text/stream',
      'Cache-Control': 'no-cache',
      Connection: 'keep-alive',
    },
  });
}

// æµå¼é—®é¢˜åˆ†æ
async function streamAnalysis(
  controller: ReadableStreamDefaultController,
  encoder: TextEncoder,
  llm: ChatOpenAI,
  question: string,
  context: string,
  complexity: string,
  domain: string
) {
  controller.enqueue(
    encoder.encode(
      `data: ${JSON.stringify({
        content: '## ğŸ” é—®é¢˜åˆ†æ\n',
        done: false,
      })}\n\n`
    )
  );

  try {
    console.log('å¼€å§‹é—®é¢˜åˆ†æ...', { question, domain, complexity });

    const analysisPrompt = ChatPromptTemplate.fromMessages([
      SystemMessagePromptTemplate.fromTemplate(`
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„{domain}é¢†åŸŸåˆ†æå¸ˆã€‚è¯·ç®€æ´åœ°åˆ†æè¿™ä¸ªé—®é¢˜ï¼š

è¦æ±‚ï¼š
1. è¯†åˆ«é—®é¢˜çš„æ ¸å¿ƒè¦ç´ ï¼ˆ1-2å¥è¯ï¼‰
2. ç¡®å®šæ¨ç†æ–¹æ³•ï¼ˆ1å¥è¯ï¼‰
3. é¢„ä¼°æ­¥éª¤æ•°é‡ï¼ˆæ ¹æ®{complexity}çº§åˆ«ï¼‰

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œç®€æ´æ˜äº†ã€‚
      `),
      HumanMessagePromptTemplate.fromTemplate(`
ä¸Šä¸‹æ–‡ï¼š{context}
é—®é¢˜ï¼š{question}

è¯·åˆ†æè¿™ä¸ªé—®é¢˜ã€‚
      `),
    ]);

    const chain = analysisPrompt.pipe(llm).pipe(new StringOutputParser());

    console.log('æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œé—®é¢˜åˆ†æ...');

    // æ·»åŠ è¶…æ—¶å¤„ç†
    const streamPromise = chain.stream({
      question,
      context: context || 'æ— ç‰¹å®šä¸Šä¸‹æ–‡',
      complexity,
      domain,
    });

    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(() => reject(new Error('é—®é¢˜åˆ†æè¶…æ—¶')), 15000); // 15ç§’è¶…æ—¶
    });

    const stream = await Promise.race([streamPromise, timeoutPromise]);

    console.log('LLMå“åº”æˆåŠŸï¼Œå¼€å§‹å¤„ç†æµå¼æ•°æ®...');

    let chunkCount = 0;
    for await (const chunk of stream) {
      chunkCount++;
      console.log(`æ”¶åˆ°ç¬¬${chunkCount}ä¸ªchunk:`, chunk);

      controller.enqueue(
        encoder.encode(
          `data: ${JSON.stringify({
            content: chunk,
            done: false,
          })}\n\n`
        )
      );
    }

    console.log(`é—®é¢˜åˆ†æå®Œæˆï¼Œå…±å¤„ç†${chunkCount}ä¸ªchunks`);

    controller.enqueue(
      encoder.encode(
        `data: ${JSON.stringify({
          content: '\n\n',
          done: false,
        })}\n\n`
      )
    );
  } catch (error) {
    console.error('é—®é¢˜åˆ†æå‡ºé”™:', error);

    // å‘é€é”™è¯¯ä¿¡æ¯
    controller.enqueue(
      encoder.encode(
        `data: ${JSON.stringify({
          content: `âŒ é—®é¢˜åˆ†æå¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}\n\n`,
          done: false,
        })}\n\n`
      )
    );

    throw error; // é‡æ–°æŠ›å‡ºé”™è¯¯è®©ä¸Šå±‚å¤„ç†
  }
}

// æµå¼é€æ­¥æ¨ç†
async function streamReasoning(
  controller: ReadableStreamDefaultController,
  encoder: TextEncoder,
  llm: ChatOpenAI,
  question: string,
  complexity: string,
  domain: string
): Promise<string> {
  controller.enqueue(
    encoder.encode(
      `data: ${JSON.stringify({
        content: '## ğŸ§  æ¨ç†è¿‡ç¨‹\n',
        done: false,
      })}\n\n`
    )
  );

  try {
    const stepCount =
      complexity === 'simple' ? 2 : complexity === 'medium' ? 4 : 6;

    console.log('å¼€å§‹é€æ­¥æ¨ç†...', { question, domain, complexity, stepCount });

    const reasoningPrompt = ChatPromptTemplate.fromMessages([
      SystemMessagePromptTemplate.fromTemplate(`
ä½ æ˜¯ä¸€ä¸ªé€»è¾‘æ¨ç†ä¸“å®¶ã€‚è¯·å¯¹è¿™ä¸ª{domain}é—®é¢˜è¿›è¡Œ{stepCount}æ­¥é€æ­¥æ¨ç†ã€‚

æ ¼å¼è¦æ±‚ï¼š
**æ­¥éª¤1**: [ç¬¬ä¸€æ­¥åˆ†æ]
**æ­¥éª¤2**: [ç¬¬äºŒæ­¥åˆ†æ]
...

æ¯æ­¥è¦ç®€æ´æœ‰åŠ›ï¼Œé€»è¾‘æ¸…æ™°ã€‚
      `),
      HumanMessagePromptTemplate.fromTemplate(`
é—®é¢˜ï¼š{question}

è¯·è¿›è¡Œé€æ­¥æ¨ç†ã€‚
      `),
    ]);

    const chain = reasoningPrompt.pipe(llm).pipe(new StringOutputParser());

    console.log('æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œé€æ­¥æ¨ç†...');

    // æ·»åŠ è¶…æ—¶å¤„ç†
    const streamPromise = chain.stream({
      question,
      domain,
      stepCount,
    });

    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(() => reject(new Error('é€æ­¥æ¨ç†è¶…æ—¶')), 45000); // 45ç§’è¶…æ—¶
    });

    const stream = await Promise.race([streamPromise, timeoutPromise]);

    console.log('æ¨ç†LLMå“åº”æˆåŠŸï¼Œå¼€å§‹å¤„ç†æµå¼æ•°æ®...');

    let fullReasoning = '';
    let chunkCount = 0;
    for await (const chunk of stream) {
      chunkCount++;
      console.log(`æ¨ç†ç¬¬${chunkCount}ä¸ªchunk:`, chunk);

      fullReasoning += chunk;
      controller.enqueue(
        encoder.encode(
          `data: ${JSON.stringify({
            content: chunk,
            done: false,
          })}\n\n`
        )
      );
    }

    console.log(`é€æ­¥æ¨ç†å®Œæˆï¼Œå…±å¤„ç†${chunkCount}ä¸ªchunks`);

    controller.enqueue(
      encoder.encode(
        `data: ${JSON.stringify({
          content: '\n\n',
          done: false,
        })}\n\n`
      )
    );

    return fullReasoning;
  } catch (error) {
    console.error('é€æ­¥æ¨ç†å‡ºé”™:', error);

    // å‘é€é”™è¯¯ä¿¡æ¯
    controller.enqueue(
      encoder.encode(
        `data: ${JSON.stringify({
          content: `âŒ æ¨ç†è¿‡ç¨‹å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}\n\n`,
          done: false,
        })}\n\n`
      )
    );

    throw error;
  }
}

// æµå¼ç»“è®ºæ€»ç»“
async function streamConclusion(
  controller: ReadableStreamDefaultController,
  encoder: TextEncoder,
  llm: ChatOpenAI,
  question: string,
  reasoning: string
) {
  controller.enqueue(
    encoder.encode(
      `data: ${JSON.stringify({
        content: '## ğŸ¯ æœ€ç»ˆç»“è®º\n',
        done: false,
      })}\n\n`
    )
  );

  try {
    console.log('å¼€å§‹ç»“è®ºæ€»ç»“...', { question });

    const conclusionPrompt = ChatPromptTemplate.fromMessages([
      SystemMessagePromptTemplate.fromTemplate(`
åŸºäºæ¨ç†è¿‡ç¨‹ï¼Œè¯·ç»™å‡ºç®€æ´çš„æœ€ç»ˆç»“è®ºã€‚

è¦æ±‚ï¼š
1. æ˜ç¡®çš„ç­”æ¡ˆ
2. ç®€è¦æ€»ç»“æ¨ç†è¦ç‚¹
3. å¯ä¿¡åº¦è¯„ä¼°(1-10åˆ†)

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œæ ¼å¼æ¸…æ™°ã€‚
      `),
      HumanMessagePromptTemplate.fromTemplate(`
é—®é¢˜ï¼š{question}
æ¨ç†è¿‡ç¨‹ï¼š{reasoning}

è¯·ç»™å‡ºæœ€ç»ˆç»“è®ºã€‚
      `),
    ]);

    const chain = conclusionPrompt.pipe(llm).pipe(new StringOutputParser());

    console.log('æ­£åœ¨è°ƒç”¨LLMè¿›è¡Œç»“è®ºæ€»ç»“...');

    // æ·»åŠ è¶…æ—¶å¤„ç†
    const streamPromise = chain.stream({
      question,
      reasoning,
    });

    const timeoutPromise = new Promise<never>((_, reject) => {
      setTimeout(() => reject(new Error('ç»“è®ºæ€»ç»“è¶…æ—¶')), 15000); // 15ç§’è¶…æ—¶
    });

    const stream = await Promise.race([streamPromise, timeoutPromise]);

    console.log('ç»“è®ºLLMå“åº”æˆåŠŸï¼Œå¼€å§‹å¤„ç†æµå¼æ•°æ®...');

    let chunkCount = 0;
    for await (const chunk of stream) {
      chunkCount++;
      console.log(`ç»“è®ºç¬¬${chunkCount}ä¸ªchunk:`, chunk);

      controller.enqueue(
        encoder.encode(
          `data: ${JSON.stringify({
            content: chunk,
            done: false,
          })}\n\n`
        )
      );
    }

    console.log(`ç»“è®ºæ€»ç»“å®Œæˆï¼Œå…±å¤„ç†${chunkCount}ä¸ªchunks`);
  } catch (error) {
    console.error('ç»“è®ºæ€»ç»“å‡ºé”™:', error);

    // å‘é€é”™è¯¯ä¿¡æ¯
    controller.enqueue(
      encoder.encode(
        `data: ${JSON.stringify({
          content: `âŒ ç»“è®ºæ€»ç»“å¤±è´¥: ${error instanceof Error ? error.message : 'æœªçŸ¥é”™è¯¯'}\n\n`,
          done: false,
        })}\n\n`
      )
    );

    throw error;
  }
}

// éæµå¼æ¨ç†å¤„ç†ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
async function performCoTReasoning(
  llm: ChatOpenAI,
  question: string,
  context: string,
  complexity: string,
  domain: string
): Promise<CoTResponse> {
  // ç®€åŒ–çš„å•æ­¥æ¨ç†
  const prompt = ChatPromptTemplate.fromMessages([
    SystemMessagePromptTemplate.fromTemplate(`
ä½ æ˜¯ä¸€ä¸ª{domain}é¢†åŸŸçš„ä¸“å®¶ã€‚è¯·å¯¹ä»¥ä¸‹é—®é¢˜è¿›è¡Œé“¾å¼æ€è€ƒæ¨ç†ï¼š

1. é¦–å…ˆåˆ†æé—®é¢˜çš„æ ¸å¿ƒè¦ç´ 
2. ç„¶åé€æ­¥æ¨ç†ï¼Œæ¯ä¸ªæ­¥éª¤è¦æœ‰æ¸…æ™°çš„é€»è¾‘
3. æœ€åå¾—å‡ºç»“è®ºå¹¶è¯„ä¼°å¯ä¿¡åº¦(1-10åˆ†)

è¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œä½¿ç”¨ä»¥ä¸‹æ ¼å¼ï¼š
**é—®é¢˜åˆ†æ**: [åˆ†æé—®é¢˜ç±»å‹å’Œå…³é”®è¦ç´ ]
**æ¨ç†æ­¥éª¤**: 
**æ­¥éª¤1**: [ç¬¬ä¸€æ­¥æ¨ç†]
**æ­¥éª¤2**: [ç¬¬äºŒæ­¥æ¨ç†]
...
**æœ€ç»ˆç»“è®º**: [æ˜ç¡®çš„ç­”æ¡ˆ]
**å¯ä¿¡åº¦è¯„ä¼°**: [1-10åˆ†åŠç†ç”±]
    `),
    HumanMessagePromptTemplate.fromTemplate(`
ä¸Šä¸‹æ–‡ï¼š{context}
å¤æ‚åº¦ï¼š{complexity}
é—®é¢˜ï¼š{question}

è¯·è¿›è¡Œé“¾å¼æ€è€ƒæ¨ç†ã€‚
    `),
  ]);

  const chain = prompt.pipe(llm).pipe(new StringOutputParser());
  const fullResponse = await chain.invoke({
    question,
    context: context || 'æ— ç‰¹å®šä¸Šä¸‹æ–‡',
    complexity,
    domain,
  });

  // ç®€å•è§£æ
  const analysisMatch = fullResponse.match(
    /\*\*é—®é¢˜åˆ†æ\*\*:\s*([\s\S]*?)(?=\*\*æ¨ç†æ­¥éª¤\*\*|$)/
  );
  const analysis = analysisMatch ? analysisMatch[1].trim() : 'åˆ†æç»“æœ';

  const reasoning_steps = extractReasoningSteps(fullResponse);
  const confidence = extractConfidence(fullResponse);

  return {
    question,
    analysis,
    reasoning_steps,
    conclusion: fullResponse,
    confidence,
    total_steps: reasoning_steps.length,
  };
}

// æå–æ¨ç†æ­¥éª¤
function extractReasoningSteps(reasoning: string): string[] {
  const steps: string[] = [];
  const stepPattern = /\*\*æ­¥éª¤ \d+\*\*:[\s\S]*?(?=\*\*æ­¥éª¤ \d+\*\*:|$)/g;
  const stepMatches = reasoning.match(stepPattern);

  if (stepMatches) {
    stepMatches.forEach((step) => {
      steps.push(step.trim());
    });
  } else {
    // å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°æ ‡å‡†æ ¼å¼ï¼ŒæŒ‰æ®µè½åˆ†å‰²
    const paragraphs = reasoning.split('\n\n').filter((p) => p.trim());
    steps.push(...paragraphs);
  }

  return steps;
}

// æå–å¯ä¿¡åº¦åˆ†æ•°
function extractConfidence(conclusion: string): number {
  const confidenceMatch = conclusion.match(/å¯ä¿¡åº¦è¯„ä¼°.*?(\d+)/);
  return confidenceMatch ? parseInt(confidenceMatch[1]) : 7;
}
